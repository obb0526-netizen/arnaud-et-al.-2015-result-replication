{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02B Automated ICA Batch Processing\n",
    "\n",
    "This notebook automates ICLabel-based ICA cleaning for the nine non-manual subjects so that downstream rheumatoid arthritis-focused ERP analyses can build on a unified preprocessing pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 06:33:54,655 [INFO] Project root resolved to /Users/leeyelim/Documents/EEG\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Ensure both project root and src directory are on sys.path for imports\n",
    "for path_candidate in (PROJECT_ROOT, PROJECT_ROOT / \"src\"):\n",
    "    path_str = str(path_candidate)\n",
    "    if path_str not in sys.path:\n",
    "        sys.path.insert(0, path_str)\n",
    "\n",
    "from src.utils.pathing import find_project_root, ensure_src_on_path\n",
    "\n",
    "project_root = find_project_root(PROJECT_ROOT)\n",
    "ensure_src_on_path()\n",
    "\n",
    "try:\n",
    "    import mne_icalabel  # noqa: F401\n",
    "    from mne_icalabel import label_components\n",
    "    ICLABEL_AVAILABLE = True\n",
    "except ModuleNotFoundError as err:\n",
    "    ICLABEL_AVAILABLE = False\n",
    "    LABEL_COMPONENTS_IMPORT_ERROR = err\n",
    "else:\n",
    "    LABEL_COMPONENTS_IMPORT_ERROR = None\n",
    "\n",
    "if ICLABEL_AVAILABLE:\n",
    "    from src.preprocessing.ica_pipeline import ICAProcessor\n",
    "else:\n",
    "    ICAProcessor = None\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"automated_ica\")\n",
    "logger.info(\"Project root resolved to %s\", project_root)\n",
    "\n",
    "if not ICLABEL_AVAILABLE:\n",
    "    logger.warning(\n",
    "        \"mne-icalabel is not installed. Install it with `pip install mne-icalabel` \"\n",
    "        \"before running the automated ICA cells.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load configuration & subject sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual ICA subject        : sub-003\n",
      "Automated ICA subject pool: ['sub-015', 'sub-006', 'sub-010', 'sub-012', 'sub-007', 'sub-002', 'sub-011', 'sub-004', 'sub-014']\n",
      "Total automated subjects  : 9\n"
     ]
    }
   ],
   "source": [
    "config_path = project_root / \"config\" / \"analysis_config.yaml\"\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "\n",
    "with open(config_path, \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "manual_subject = config[\"subjects\"][\"manual_ica_subject\"]\n",
    "selected_subjects = config[\"subjects\"][\"selected\"]\n",
    "automated_subjects = [sub for sub in selected_subjects if sub != manual_subject]\n",
    "\n",
    "print(f\"Manual ICA subject        : {manual_subject}\")\n",
    "print(f\"Automated ICA subject pool: {automated_subjects}\")\n",
    "print(f\"Total automated subjects  : {len(automated_subjects)}\")\n",
    "\n",
    "if not ICLABEL_AVAILABLE:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"mne-icalabel is required for automated ICA. Install it with `pip install mne-icalabel` \"\n",
    "        \"and rerun the notebook.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load preprocessing summary (runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ICLabel/ARTIST parameters & helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = project_root / \"data\" / \"preprocessed\" / \"preprocessing_summary.csv\"\n",
    "if not summary_path.exists():\n",
    "    raise FileNotFoundError(f\"Preprocessing summary not found at {summary_path}\")\n",
    "\n",
    "summary_df = pd.read_csv(summary_path)\n",
    "summary_df[\"subject\"] = summary_df[\"subject\"].astype(str)\n",
    "summary_df[\"session\"] = summary_df[\"session\"].astype(str)\n",
    "summary_df[\"run\"] = summary_df[\"run\"].astype(str)\n",
    "\n",
    "auto_runs_df = summary_df[summary_df[\"subject\"].isin(automated_subjects)].copy()\n",
    "auto_runs_df[\"run_index\"] = (\n",
    "    auto_runs_df[\"run\"].str.extract(r\"run-(\\d+)\", expand=False).astype(float).fillna(0).astype(int)\n",
    ")\n",
    "auto_runs_df.sort_values([\"subject\", \"session\", \"run_index\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Session-level ICA processing (merge → pre-annotate → ICA → re-ref → post-annotate → save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inventory ICA-cleaned outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session selection\n",
    "\n",
    "Choose whether to process every eligible session or focus on a single subject/session pair. Set the config cell below before running the automation section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Save automated ICA summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODE = \"all\"  # Options: \"all\" or \"single\"\n",
    "TARGET_SUBJECT = \"sub-004\"\n",
    "TARGET_SESSION = \"ses-01\"\n",
    "\n",
    "RUN_MODE = RUN_MODE.lower().strip()\n",
    "if RUN_MODE not in {\"all\", \"single\"}:\n",
    "    raise ValueError(\"RUN_MODE must be 'all' or 'single'\")\n",
    "\n",
    "if RUN_MODE == \"single\":\n",
    "    target_subject = TARGET_SUBJECT.strip()\n",
    "    target_session = TARGET_SESSION.strip()\n",
    "    if not target_subject or not target_session:\n",
    "        raise ValueError(\"TARGET_SUBJECT and TARGET_SESSION must be set when RUN_MODE='single'\")\n",
    "    if target_subject == manual_subject:\n",
    "        raise ValueError(\"sub-003 is reserved for manual ICA; choose a different subject.\")\n",
    "    mask = (auto_runs_df[\"subject\"] == target_subject) & (auto_runs_df[\"session\"] == target_session)\n",
    "    auto_runs_df = auto_runs_df[mask]\n",
    "    if auto_runs_df.empty:\n",
    "        raise ValueError(f\"No runs found for {target_subject} {target_session} in preprocessing summary.\")\n",
    "else:\n",
    "    print(f\"Processing all automated subjects (excluding manual subject {manual_subject}).\")\n",
    "\n",
    "if auto_runs_df.empty:\n",
    "    raise RuntimeError(\"No runs left to process after applying the selection mask.\")\n",
    "\n",
    "automated_session_groups = [\n",
    "    (subject, session, group.reset_index(drop=True))\n",
    "    for (subject, session), group in auto_runs_df.groupby([\"subject\", \"session\"])\n",
    "]\n",
    "\n",
    "print(f\"Total automated sessions in scope: {len(automated_session_groups)}\")\n",
    "print(\"Runs per subject/session:\")\n",
    "display(\n",
    "    auto_runs_df.groupby([\"subject\", \"session\"])[\"run\"].count().rename(\"n_runs\")\n",
    ")\n",
    "\n",
    "auto_runs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cohen's d per session (time-resolved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICLABEL_CLASSES = [\n",
    "    \"brain\",\n",
    "    \"muscle\",\n",
    "    \"eye\",\n",
    "    \"heart\",\n",
    "    \"line_noise\",\n",
    "    \"channel_noise\",\n",
    "    \"other\",\n",
    "]\n",
    "\n",
    "REJECT_TARGETS = {\n",
    "    (label.lower().strip().replace(\" \", \"_\"))\n",
    "    for label in config[\"preprocessing\"][\"iclabel\"][\"reject_classes\"]\n",
    "}\n",
    "REJECT_TARGETS.discard(\"brain\")\n",
    "\n",
    "THRESHOLD = config[\"preprocessing\"][\"iclabel\"][\"threshold\"]\n",
    "OUTPUT_ROOT = project_root / \"data\" / \"preprocessed\" / \"after_ica\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR = project_root / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ARTIST_CFG = config[\"preprocessing\"].get(\"artist\", {})\n",
    "REJECT_CRITERIA = ARTIST_CFG.get(\"reject_criteria\", {})\n",
    "\n",
    "\n",
    "def _to_float(value, default):\n",
    "    if value is None:\n",
    "        return default\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return default\n",
    "\n",
    "\n",
    "AMPLITUDE_THRESHOLD_UV = _to_float(REJECT_CRITERIA.get(\"eeg\", 150e-6), 150e-6) * 1e6\n",
    "MIN_BAD_DURATION_SEC = _to_float(ARTIST_CFG.get(\"min_duration_sec\", 0.5), 0.5)\n",
    "\n",
    "\n",
    "def canonicalize(label: str) -> str:\n",
    "    if not label:\n",
    "        return \"unknown\"\n",
    "    return label.lower().strip().replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "def build_session_output_paths(subject: str, session: str) -> dict:\n",
    "    session_dir = OUTPUT_ROOT / subject / session\n",
    "    session_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    stem = f\"{subject}_{session}_preprocessed_ica\"\n",
    "    return {\n",
    "        \"cleaned\": session_dir / f\"{stem}_cleaned.fif\",\n",
    "        \"ica_model\": session_dir / f\"{stem}_model.fif\",\n",
    "        \"results\": session_dir / f\"{stem}_results.json\",\n",
    "        \"rejection\": session_dir / f\"{stem}_component_rejection.json\",\n",
    "    }\n",
    "\n",
    "\n",
    "def merge_session_runs(session_df: pd.DataFrame):\n",
    "    raws = []\n",
    "    paths = []\n",
    "    for _, row in session_df.sort_values(\"run_index\").iterrows():\n",
    "        run_path = Path(row[\"reref_path\"])\n",
    "        if not run_path.exists():\n",
    "            raise FileNotFoundError(f\"Re-referenced file missing: {run_path}\")\n",
    "        raw = mne.io.read_raw_fif(run_path, preload=True, verbose=\"ERROR\")\n",
    "        raw.load_data()\n",
    "        raws.append(raw)\n",
    "        paths.append(run_path)\n",
    "\n",
    "    if not raws:\n",
    "        raise RuntimeError(\"No runs available for session merge\")\n",
    "\n",
    "    if len(raws) == 1:\n",
    "        merged = raws[0]\n",
    "    else:\n",
    "        merged = mne.concatenate_raws(raws, preload=True, verbose=False)\n",
    "\n",
    "    return merged, paths\n",
    "\n",
    "\n",
    "def _mask_to_segments(mask: np.ndarray, sfreq: float, min_samples: int):\n",
    "    if min_samples <= 0:\n",
    "        min_samples = 1\n",
    "    padded = np.concatenate(([0], mask.astype(int), [0]))\n",
    "    changes = np.diff(padded)\n",
    "    starts = np.where(changes == 1)[0]\n",
    "    ends = np.where(changes == -1)[0]\n",
    "    for start, end in zip(starts, ends):\n",
    "        if end - start >= min_samples:\n",
    "            onset = start / sfreq\n",
    "            duration = (end - start) / sfreq\n",
    "            yield onset, duration\n",
    "\n",
    "\n",
    "def annotate_high_amplitude(\n",
    "    raw: mne.io.Raw,\n",
    "    *,\n",
    "    label: str,\n",
    "    threshold_uv: float = AMPLITUDE_THRESHOLD_UV,\n",
    "    min_duration_sec: float = MIN_BAD_DURATION_SEC,\n",
    ") -> dict:\n",
    "    if threshold_uv <= 0:\n",
    "        return {\"label\": label, \"n_segments\": 0, \"total_duration\": 0.0}\n",
    "\n",
    "    picks = mne.pick_types(raw.info, eeg=True, exclude=[])\n",
    "    if len(picks) == 0:\n",
    "        return {\"label\": label, \"n_segments\": 0, \"total_duration\": 0.0}\n",
    "\n",
    "    data = raw.get_data(picks=picks)\n",
    "    envelope = np.max(np.abs(data), axis=0)\n",
    "    mask = envelope > (threshold_uv * 1e-6)\n",
    "    min_samples = int(max(1, min_duration_sec * raw.info[\"sfreq\"]))\n",
    "    segments = list(_mask_to_segments(mask, raw.info[\"sfreq\"], min_samples))\n",
    "\n",
    "    if segments:\n",
    "        new_annotations = mne.Annotations(\n",
    "            onset=[seg[0] for seg in segments],\n",
    "            duration=[seg[1] for seg in segments],\n",
    "            description=[label] * len(segments),\n",
    "        )\n",
    "        if raw.annotations is None:\n",
    "            raw.set_annotations(new_annotations)\n",
    "        else:\n",
    "            raw.set_annotations(raw.annotations + new_annotations)\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"n_segments\": len(segments),\n",
    "        \"total_duration\": float(sum(seg[1] for seg in segments)),\n",
    "        \"threshold_uv\": threshold_uv,\n",
    "        \"min_duration_s\": min_duration_sec,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_component_probabilities(scores_row, class_order=ICLABEL_CLASSES) -> dict:\n",
    "    arr = np.asarray(scores_row).ravel()\n",
    "    if arr.size == 0:\n",
    "        return {}\n",
    "    if arr.size == len(class_order):\n",
    "        return {cls: float(arr[idx]) for idx, cls in enumerate(class_order)}\n",
    "    if arr.size == 1:\n",
    "        return {class_order[0]: float(arr[0])}\n",
    "    limit = min(len(class_order), arr.size)\n",
    "    return {class_order[idx]: float(arr[idx]) for idx in range(limit)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.set_log_level(\"WARNING\")\n",
    "ica_processor = ICAProcessor(config)\n",
    "\n",
    "records = []\n",
    "failures = []\n",
    "\n",
    "for subject, session, session_df in automated_session_groups:\n",
    "    session_label = f\"{subject}_{session}\"\n",
    "    logger.info(\"Processing %s (%d runs)\", session_label, len(session_df))\n",
    "\n",
    "    try:\n",
    "        raw, run_paths = merge_session_runs(session_df)\n",
    "        raw.load_data()\n",
    "\n",
    "        pre_annotation = annotate_high_amplitude(raw, label=\"BAD_PRE_ICA\")\n",
    "        ica = ica_processor.run_ica(raw.copy(), subject=session_label)\n",
    "\n",
    "        reject_components, classification_results = ica_processor.automated_ica_rejection(\n",
    "            raw, ica, session_label\n",
    "        )\n",
    "\n",
    "        raw_cleaned = ica_processor.apply_ica_rejection(raw, ica, reject_components)\n",
    "        raw_cleaned.load_data()\n",
    "        raw_cleaned.set_eeg_reference(\"average\", projection=False)\n",
    "        post_annotation = annotate_high_amplitude(raw_cleaned, label=\"BAD_POST_ICA\")\n",
    "\n",
    "        output_paths = build_session_output_paths(subject, session)\n",
    "        raw_cleaned.save(output_paths[\"cleaned\"], overwrite=True, verbose=\"ERROR\")\n",
    "        ica.save(output_paths[\"ica_model\"], overwrite=True)\n",
    "\n",
    "        class_order = classification_results.get(\"classes\", ICLABEL_CLASSES)\n",
    "        decision_threshold = classification_results.get(\"rejection_threshold\", THRESHOLD)\n",
    "\n",
    "        classification_payload = classification_results.copy()\n",
    "        classification_payload.update(\n",
    "            {\n",
    "                \"subject\": subject,\n",
    "                \"session\": session,\n",
    "                \"run\": \"merged_session\",\n",
    "                \"run_label\": session_label,\n",
    "                \"n_runs_merged\": len(session_df),\n",
    "                \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                \"class_order\": class_order,\n",
    "                \"threshold\": decision_threshold,\n",
    "                \"reject_targets\": classification_results.get(\n",
    "                    \"rejected_classes\", sorted(REJECT_TARGETS)\n",
    "                ),\n",
    "                \"annotation_summary\": {\n",
    "                    \"pre_ica\": pre_annotation,\n",
    "                    \"post_ica\": post_annotation,\n",
    "                },\n",
    "                \"source_runs\": [str(path) for path in run_paths],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        with open(output_paths[\"results\"], \"w\") as f:\n",
    "            json.dump(classification_payload, f, indent=2)\n",
    "\n",
    "        rejection_payload = {\n",
    "            \"subject\": subject,\n",
    "            \"session\": session,\n",
    "            \"run\": \"merged_session\",\n",
    "            \"n_components\": int(ica.n_components_),\n",
    "            \"rejected_components\": list(reject_components),\n",
    "            \"n_rejected\": len(reject_components),\n",
    "            \"method\": \"iclabel_automated\",\n",
    "            \"threshold\": decision_threshold,\n",
    "            \"n_runs_merged\": len(session_df),\n",
    "        }\n",
    "\n",
    "        with open(output_paths[\"rejection\"], \"w\") as f:\n",
    "            json.dump(rejection_payload, f, indent=2)\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"subject\": subject,\n",
    "                \"session\": session,\n",
    "                \"run\": \"merged_session\",\n",
    "                \"n_runs_merged\": len(session_df),\n",
    "                \"n_components\": int(ica.n_components_),\n",
    "                \"n_rejected\": len(reject_components),\n",
    "                \"threshold\": decision_threshold,\n",
    "                \"cleaned_path\": str(output_paths[\"cleaned\"]),\n",
    "                \"ica_model_path\": str(output_paths[\"ica_model\"]),\n",
    "                \"classification_path\": str(output_paths[\"results\"]),\n",
    "                \"rejection_log_path\": str(output_paths[\"rejection\"]),\n",
    "                \"timestamp_utc\": classification_payload[\"timestamp_utc\"],\n",
    "                \"pre_bad_segments\": pre_annotation[\"n_segments\"],\n",
    "                \"pre_bad_seconds\": pre_annotation[\"total_duration\"],\n",
    "                \"post_bad_segments\": post_annotation[\"n_segments\"],\n",
    "                \"post_bad_seconds\": post_annotation[\"total_duration\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            \"Completed %s → %d/%d components rejected\",\n",
    "            session_label,\n",
    "            len(reject_components),\n",
    "            int(ica.n_components_),\n",
    "        )\n",
    "\n",
    "    except Exception as exc:  # pylint: disable=broad-except\n",
    "        logger.exception(\"Failed to process %s\", session_label)\n",
    "        failures.append(\n",
    "            {\n",
    "                \"session_label\": session_label,\n",
    "                \"reason\": type(exc).__name__,\n",
    "                \"message\": str(exc),\n",
    "            }\n",
    "        )\n",
    "\n",
    "if failures:\n",
    "    logger.warning(\"Encountered %d failures during automated ICA\", len(failures))\n",
    "    failures\n",
    "else:\n",
    "    logger.info(\"Automated ICA completed for all sessions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Group-level ERP metrics (SNR, SME, peak d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved group-condition SME → /Users/leeyelim/Documents/EEG/results/group_erp_sme_by_condition.csv (rows=40)\n",
      "Saved group-condition SME summary → /Users/leeyelim/Documents/EEG/results/group_erp_sme_by_condition_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>ROI</th>\n",
       "      <th>SME_familiar_before_uV</th>\n",
       "      <th>SME_familiar_after_uV</th>\n",
       "      <th>Delta_SME_familiar_uV</th>\n",
       "      <th>SME_new_before_uV</th>\n",
       "      <th>SME_new_after_uV</th>\n",
       "      <th>Delta_SME_new_uV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>Frontal ROI</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>0.353415</td>\n",
       "      <td>-0.590687</td>\n",
       "      <td>0.987072</td>\n",
       "      <td>0.283027</td>\n",
       "      <td>-0.704044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>Parietal ROI</td>\n",
       "      <td>0.213074</td>\n",
       "      <td>0.180250</td>\n",
       "      <td>-0.032824</td>\n",
       "      <td>0.201076</td>\n",
       "      <td>0.166987</td>\n",
       "      <td>-0.034089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-02</td>\n",
       "      <td>Frontal ROI</td>\n",
       "      <td>0.998758</td>\n",
       "      <td>0.372097</td>\n",
       "      <td>-0.626661</td>\n",
       "      <td>1.045773</td>\n",
       "      <td>0.291014</td>\n",
       "      <td>-0.754759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-02</td>\n",
       "      <td>Parietal ROI</td>\n",
       "      <td>0.226218</td>\n",
       "      <td>0.189009</td>\n",
       "      <td>-0.037208</td>\n",
       "      <td>0.212971</td>\n",
       "      <td>0.175413</td>\n",
       "      <td>-0.037557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-003</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>Frontal ROI</td>\n",
       "      <td>1.233615</td>\n",
       "      <td>0.885281</td>\n",
       "      <td>-0.348334</td>\n",
       "      <td>2.306136</td>\n",
       "      <td>0.920543</td>\n",
       "      <td>-1.385593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject session           ROI  SME_familiar_before_uV  \\\n",
       "0  sub-002  ses-01   Frontal ROI                0.944103   \n",
       "1  sub-002  ses-01  Parietal ROI                0.213074   \n",
       "2  sub-002  ses-02   Frontal ROI                0.998758   \n",
       "3  sub-002  ses-02  Parietal ROI                0.226218   \n",
       "4  sub-003  ses-01   Frontal ROI                1.233615   \n",
       "\n",
       "   SME_familiar_after_uV  Delta_SME_familiar_uV  SME_new_before_uV  \\\n",
       "0               0.353415              -0.590687           0.987072   \n",
       "1               0.180250              -0.032824           0.201076   \n",
       "2               0.372097              -0.626661           1.045773   \n",
       "3               0.189009              -0.037208           0.212971   \n",
       "4               0.885281              -0.348334           2.306136   \n",
       "\n",
       "   SME_new_after_uV  Delta_SME_new_uV  \n",
       "0          0.283027         -0.704044  \n",
       "1          0.166987         -0.034089  \n",
       "2          0.291014         -0.754759  \n",
       "3          0.175413         -0.037557  \n",
       "4          0.920543         -1.385593  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.2a. Group-level SME by condition with ROI-specific window override\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "pre_root = project_root / 'data' / 'preprocessed' / 'after_rereferencing'\n",
    "after_root = project_root / 'data' / 'preprocessed' / 'after_ica'\n",
    "raw_events_root = project_root / 'ds002680'\n",
    "VOLTS_TO_UV = 1e6\n",
    "\n",
    "# Defaults from config\n",
    "erp_cfg = config.get('erp_analysis', {})\n",
    "baseline = tuple(erp_cfg.get('baseline', [-0.1, 0.0]))\n",
    "post_window_default = tuple(erp_cfg.get('post_window', [0.3, 0.5]))\n",
    "\n",
    "roi_map = {\n",
    "    'Frontal ROI': ['FP1', 'FP2'],\n",
    "    'Parietal ROI': ['P3', 'P3\"', 'P4', 'P4\"', 'PZ', 'PZ\"', 'CZ'],\n",
    "}\n",
    "roi_cfg = erp_cfg.get('roi', {})\n",
    "\n",
    "familiar_labels = {'animal_target','nonanimal_target','easy_target','difficult_target'}\n",
    "new_labels = {'animal_distractor','nonanimal_distractor','easy_distractor','difficult_distractor'}\n",
    "valid_labels = familiar_labels | new_labels\n",
    "\n",
    "\n",
    "def _window_for_roi(roi_label: str) -> Tuple[float, float]:\n",
    "    return (0.4, 0.5) if 'Parieto' in roi_label else post_window_default\n",
    "\n",
    "\n",
    "def _load_run_events(subject: str, session: str, run_token: str, sfreq: float) -> np.ndarray:\n",
    "    f = raw_events_root/subject/session/'eeg'/f\"{subject}_{session}_task-gonogo_{run_token}_events.tsv\"\n",
    "    if not f.exists():\n",
    "        return np.empty((0,3), dtype=int)\n",
    "    df = pd.read_csv(f, sep='\\t')\n",
    "    df = df[df['value'].isin(valid_labels)]\n",
    "    if df.empty:\n",
    "        return np.empty((0,3), dtype=int)\n",
    "    samples = (df['onset'].values * sfreq).round().astype(int)\n",
    "    codes = np.array([1 if v in familiar_labels else 2 for v in df['value']], dtype=int)\n",
    "    return np.column_stack([samples, np.zeros(len(samples), dtype=int), codes])\n",
    "\n",
    "\n",
    "def _merge_stage(subject: str, session: str, stage_root: Path, suffix: str) -> Optional[mne.io.Raw]:\n",
    "    ses_dir = stage_root/subject/session\n",
    "    if not ses_dir.exists():\n",
    "        return None\n",
    "    if suffix == 'ica_cleaned':\n",
    "        cand = sorted(ses_dir.glob(f\"{subject}_{session}_preprocessed_ica*_cleaned.fif\"))\n",
    "        if cand:\n",
    "            raw = mne.io.read_raw_fif(str(cand[0]), preload=True, verbose='ERROR'); raw.load_data(); return raw\n",
    "    run_files = sorted(ses_dir.glob(f\"{subject}_{session}_run-*_preprocessed_{suffix}.fif\"))\n",
    "    if not run_files:\n",
    "        return None\n",
    "    raws = [mne.io.read_raw_fif(str(p), preload=True, verbose='ERROR') for p in run_files]\n",
    "    [r.load_data() for r in raws]\n",
    "    return raws[0] if len(raws)==1 else mne.concatenate_raws(raws, preload=True, verbose=False)\n",
    "\n",
    "\n",
    "def _build_session_events(subject: str, session: str, sfreq: float) -> np.ndarray:\n",
    "    ses_dir = pre_root/subject/session\n",
    "    run_files = sorted(ses_dir.glob(f\"{subject}_{session}_run-*_preprocessed_after_rereferencing.fif\"))\n",
    "    if not run_files:\n",
    "        return np.empty((0,3), dtype=int)\n",
    "    evs, off = [], 0\n",
    "    for p in run_files:\n",
    "        raw_tmp = mne.io.read_raw_fif(str(p), preload=False, verbose='ERROR')\n",
    "        run_tok = p.stem.split('run-')[-1].split('_')[0]\n",
    "        e = _load_run_events(subject, session, f\"run-{run_tok}\", raw_tmp.info['sfreq'])\n",
    "        if e.size:\n",
    "            e[:,0] += off\n",
    "            evs.append(e)\n",
    "        off += raw_tmp.n_times\n",
    "    if not evs:\n",
    "        return np.empty((0,3), dtype=int)\n",
    "    e = np.vstack(evs)\n",
    "    return e[np.argsort(e[:,0])]\n",
    "\n",
    "\n",
    "def _pick_roi(raw: mne.io.Raw, roi_label: str):\n",
    "    desired = roi_map.get(roi_label, [])\n",
    "    present = [ch for ch in desired if ch in raw.ch_names]\n",
    "    if present:\n",
    "        return mne.pick_channels(raw.ch_names, present, ordered=True), present\n",
    "    key = 'parietal' if 'Parieto' in roi_label else 'frontal'\n",
    "    fb = [ch for ch in roi_cfg.get(key, []) if ch in raw.ch_names]\n",
    "    return (mne.pick_channels(raw.ch_names, fb, ordered=True), fb) if fb else (np.array([], dtype=int), [])\n",
    "\n",
    "\n",
    "def _epochs(raw: mne.io.Raw, events: np.ndarray) -> Optional[mne.Epochs]:\n",
    "    if events.size == 0:\n",
    "        return None\n",
    "    try:\n",
    "        return mne.Epochs(raw, events, event_id={'familiar':1,'new':2}, tmin=-0.2, tmax=0.6,\n",
    "                          baseline=baseline, preload=True, verbose='ERROR', event_repeated='drop')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _sme_condition(epochs: Optional[mne.Epochs], picks: np.ndarray, window: Tuple[float,float], condition: str) -> float:\n",
    "    if epochs is None or len(picks)==0:\n",
    "        return float('nan')\n",
    "    try:\n",
    "        sel = epochs[condition]\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "    if len(sel)==0:\n",
    "        return float('nan')\n",
    "    t = sel.times\n",
    "    post = (t >= window[0]) & (t <= window[1])\n",
    "    if not np.any(post):\n",
    "        return float('nan')\n",
    "    X = sel.get_data()[:, picks, :].mean(axis=1)\n",
    "    trial_vals = X[:, post].mean(axis=1)\n",
    "    n = max(len(trial_vals), 1)\n",
    "    ddof = 1 if n>1 else 0\n",
    "    return float((np.std(trial_vals, ddof=ddof) / np.sqrt(n)) * VOLTS_TO_UV)\n",
    "\n",
    "# Build sessions list\n",
    "if 'summary_df' in globals():\n",
    "    sessions = sorted({(r['subject'], r['session']) for _, r in summary_df.iterrows()})\n",
    "else:\n",
    "    sessions = []\n",
    "    for subj_dir in sorted((pre_root).glob('sub-*')):\n",
    "        for ses_dir in sorted(subj_dir.glob('ses-*')):\n",
    "            sessions.append((subj_dir.name, ses_dir.name))\n",
    "\n",
    "cond_records = []\n",
    "for subject, session in sessions:\n",
    "    raw_b = _merge_stage(subject, session, pre_root, 'after_rereferencing')\n",
    "    raw_a = _merge_stage(subject, session, after_root, 'ica_cleaned')\n",
    "    if raw_b is None or raw_a is None:\n",
    "        continue\n",
    "    ev = _build_session_events(subject, session, raw_b.info['sfreq'])\n",
    "    if ev.size == 0:\n",
    "        continue\n",
    "    ep_b = _epochs(raw_b, ev)\n",
    "    ep_a = _epochs(raw_a, ev)\n",
    "    for roi_label in roi_map:\n",
    "        win = _window_for_roi(roi_label)\n",
    "        pb, used_b = _pick_roi(raw_b, roi_label)\n",
    "        pa, used_a = _pick_roi(raw_a, roi_label)\n",
    "        if len(pb)==0 or len(pa)==0:\n",
    "            continue\n",
    "        sme_f_b = _sme_condition(ep_b, pb, win, 'familiar')\n",
    "        sme_n_b = _sme_condition(ep_b, pb, win, 'new')\n",
    "        sme_f_a = _sme_condition(ep_a, pa, win, 'familiar')\n",
    "        sme_n_a = _sme_condition(ep_a, pa, win, 'new')\n",
    "        cond_records.append({\n",
    "            'subject': subject,\n",
    "            'session': session,\n",
    "            'ROI': roi_label,\n",
    "            'SME_familiar_before_uV': sme_f_b,\n",
    "            'SME_familiar_after_uV': sme_f_a,\n",
    "            'Delta_SME_familiar_uV': (sme_f_a - sme_f_b) if (np.isfinite(sme_f_a) and np.isfinite(sme_f_b)) else np.nan,\n",
    "            'SME_new_before_uV': sme_n_b,\n",
    "            'SME_new_after_uV': sme_n_a,\n",
    "            'Delta_SME_new_uV': (sme_n_a - sme_n_b) if (np.isfinite(sme_n_a) and np.isfinite(sme_n_b)) else np.nan,\n",
    "        })\n",
    "\n",
    "cond_df = pd.DataFrame(cond_records)\n",
    "out_path = RESULTS_DIR / 'group_erp_sme_by_condition.csv'\n",
    "cond_df.to_csv(out_path, index=False)\n",
    "print(f'Saved group-condition SME → {out_path} (rows={len(cond_df)})')\n",
    "\n",
    "if not cond_df.empty:\n",
    "    summ = cond_df.groupby('ROI')[[\n",
    "        'SME_familiar_before_uV','SME_familiar_after_uV','Delta_SME_familiar_uV',\n",
    "        'SME_new_before_uV','SME_new_after_uV','Delta_SME_new_uV']].mean().reset_index()\n",
    "    summ_path = RESULTS_DIR / 'group_erp_sme_by_condition_summary.csv'\n",
    "    summ.to_csv(summ_path, index=False)\n",
    "    print(f'Saved group-condition SME summary → {summ_path}')\n",
    "\n",
    "cond_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ICA cleaned inventory → /Users/leeyelim/Documents/EEG/results/ica_cleaned_inventory.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>run</th>\n",
       "      <th>cleaned_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>run-10</td>\n",
       "      <td>/Users/leeyelim/Documents/EEG/data/preprocesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>run-11</td>\n",
       "      <td>/Users/leeyelim/Documents/EEG/data/preprocesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>run-12</td>\n",
       "      <td>/Users/leeyelim/Documents/EEG/data/preprocesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>run-13</td>\n",
       "      <td>/Users/leeyelim/Documents/EEG/data/preprocesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-002</td>\n",
       "      <td>ses-01</td>\n",
       "      <td>run-1</td>\n",
       "      <td>/Users/leeyelim/Documents/EEG/data/preprocesse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject session     run                                       cleaned_path\n",
       "0  sub-002  ses-01  run-10  /Users/leeyelim/Documents/EEG/data/preprocesse...\n",
       "1  sub-002  ses-01  run-11  /Users/leeyelim/Documents/EEG/data/preprocesse...\n",
       "2  sub-002  ses-01  run-12  /Users/leeyelim/Documents/EEG/data/preprocesse...\n",
       "3  sub-002  ses-01  run-13  /Users/leeyelim/Documents/EEG/data/preprocesse...\n",
       "4  sub-002  ses-01   run-1  /Users/leeyelim/Documents/EEG/data/preprocesse..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "inventory_records = []\n",
    "for subject_dir in sorted((OUTPUT_ROOT).glob('sub-*')):\n",
    "    subject = subject_dir.name\n",
    "    for session_dir in sorted(subject_dir.glob('ses-*')):\n",
    "        session = session_dir.name\n",
    "        cleaned_files = sorted(session_dir.glob(f\"{subject}_{session}_*preprocessed_ica*_cleaned*.fif\"))\n",
    "        for path in cleaned_files:\n",
    "            name = path.stem\n",
    "            match = re.search(r'run-(\\d+)', name)\n",
    "            run_label = f\"run-{match.group(1)}\" if match else 'merged_session'\n",
    "            inventory_records.append(\n",
    "                {\n",
    "                    'subject': subject,\n",
    "                    'session': session,\n",
    "                    'run': run_label,\n",
    "                    'cleaned_path': str(path),\n",
    "                }\n",
    "            )\n",
    "\n",
    "inventory_df = pd.DataFrame(inventory_records)\n",
    "if inventory_df.empty:\n",
    "    raise RuntimeError('No ICA-cleaned files found under data/preprocessed/after_ica')\n",
    "\n",
    "inventory_output = RESULTS_DIR / 'ica_cleaned_inventory.csv'\n",
    "inventory_df.to_csv(inventory_output, index=False)\n",
    "print(f\"Saved ICA cleaned inventory → {inventory_output}\")\n",
    "inventory_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before/After ICA SNR Validation\n",
    "\n",
    "The plan requires logging the SNR (in dB) before and after ICA for every subject. The cell below uses `ICAValidator` to compute the ERP-based SNR metric on the re-referenced data and the matching ICA-cleaned outputs. Results are stored under `results/ica_validation/ica_snr_summary.csv` for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 06:35:56,773 [INFO] EEGDataLoader initialized\n",
      "  Project root: /Users/leeyelim/Documents/EEG\n",
      "  Config: /Users/leeyelim/Documents/EEG/config/analysis_config.yaml\n",
      "  Raw dir: /Users/leeyelim/Documents/EEG/ds002680 (exists=True)\n",
      "  Preprocessed dir: /Users/leeyelim/Documents/EEG/data/preprocessed (exists=True)\n",
      "  Derivatives dir: /Users/leeyelim/Documents/EEG/data/derivatives (exists=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summary_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mica_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ICAValidator\n\u001b[1;32m      3\u001b[0m validator \u001b[38;5;241m=\u001b[39m ICAValidator(config)\n\u001b[1;32m      4\u001b[0m ica_snr_summary \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mcompute_snr_summary(\n\u001b[1;32m      5\u001b[0m     subjects\u001b[38;5;241m=\u001b[39mautomated_subjects,\n\u001b[0;32m----> 6\u001b[0m     summary_csv\u001b[38;5;241m=\u001b[39m\u001b[43msummary_path\u001b[49m,\n\u001b[1;32m      7\u001b[0m     save_path\u001b[38;5;241m=\u001b[39mproject_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mica_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mica_snr_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m ica_snr_summary\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_path' is not defined"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.ica_validation import ICAValidator\n",
    "\n",
    "validator = ICAValidator(config)\n",
    "ica_snr_summary = validator.compute_snr_summary(\n",
    "    subjects=automated_subjects,\n",
    "    summary_csv=summary_path,\n",
    "    save_path=project_root / \"results\" / \"ica_validation\" / \"ica_snr_summary.csv\"\n",
    ")\n",
    "\n",
    "ica_snr_summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrecords\u001b[49m:\n\u001b[1;32m      2\u001b[0m     automated_summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(records)\n\u001b[1;32m      3\u001b[0m     summary_output \u001b[38;5;241m=\u001b[39m RESULTS_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mica_automated_summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'records' is not defined"
     ]
    }
   ],
   "source": [
    "if records:\n",
    "    automated_summary = pd.DataFrame(records)\n",
    "    summary_output = RESULTS_DIR / \"ica_automated_summary.csv\"\n",
    "    automated_summary.to_csv(summary_output, index=False)\n",
    "    print(f\"Saved automated ICA summary → {summary_output}\")\n",
    "    automated_summary\n",
    "else:\n",
    "    print(\"No automated ICA runs completed. Check the failure log above for details.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m pre_root \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_rereferencing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m after_root \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_ica\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m summary_session_counts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 23\u001b[0m     \u001b[43msummary_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_runs\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_run_events\u001b[39m(subject: \u001b[38;5;28mstr\u001b[39m, session: \u001b[38;5;28mstr\u001b[39m, run_token: \u001b[38;5;28mstr\u001b[39m, sfreq: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     28\u001b[0m     events_file \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m         raw_events_root\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;241m/\u001b[39m subject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_task-gonogo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_events.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_df' is not defined"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "requested_roi = {\n",
    "    'Frontal ROI': ['FP1', 'FP2'],\n",
    "    'Parietal ROI': ['P3', 'P3\"', 'P4', 'P4\"', 'PZ', 'PZ\"', 'CZ'],\n",
    "}\n",
    "config_roi = config['erp_analysis']['roi']\n",
    "\n",
    "familiar_labels = {\n",
    "    'animal_target', 'nonanimal_target', 'easy_target', 'difficult_target'\n",
    "}\n",
    "new_labels = {\n",
    "    'animal_distractor', 'nonanimal_distractor', 'easy_distractor', 'difficult_distractor'\n",
    "}\n",
    "valid_labels = familiar_labels | new_labels\n",
    "raw_events_root = project_root / 'ds002680'\n",
    "pre_root = project_root / 'data' / 'preprocessed' / 'after_rereferencing'\n",
    "after_root = project_root / 'data' / 'preprocessed' / 'after_ica'\n",
    "\n",
    "summary_session_counts = (\n",
    "    summary_df.groupby(['subject', 'session']).size().rename('n_runs').to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def load_run_events(subject: str, session: str, run_token: str, sfreq: float) -> np.ndarray:\n",
    "    events_file = (\n",
    "        raw_events_root\n",
    "        / subject\n",
    "        / session\n",
    "        / 'eeg'\n",
    "        / f\"{subject}_{session}_task-gonogo_{run_token}_events.tsv\"\n",
    "    )\n",
    "    if not events_file.exists():\n",
    "        raise FileNotFoundError(f\"Events TSV missing: {events_file}\")\n",
    "    df = pd.read_csv(events_file, sep='\t')\n",
    "    stim_df = df[df['value'].isin(valid_labels)].copy()\n",
    "    if stim_df.empty:\n",
    "        return np.empty((0, 3), dtype=int)\n",
    "    samples = (stim_df['onset'].values * sfreq).round().astype(int)\n",
    "    codes = np.array([1 if val in familiar_labels else 2 for val in stim_df['value']], dtype=int)\n",
    "    events = np.column_stack([samples, np.zeros(len(samples), dtype=int), codes])\n",
    "    return events\n",
    "\n",
    "\n",
    "def merge_stage(subject: str, session: str, stage_root: Path, suffix: str) -> mne.io.Raw:\n",
    "    session_dir = stage_root / subject / session\n",
    "    if not session_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing stage directory: {session_dir}\")\n",
    "\n",
    "    candidates = sorted(session_dir.glob(f\"{subject}_{session}_preprocessed_ica*_cleaned.fif\"))\n",
    "    if suffix == 'ica_cleaned' and candidates:\n",
    "        raw = mne.io.read_raw_fif(str(candidates[0]), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        return raw\n",
    "\n",
    "    pattern = f\"{subject}_{session}_run-*_preprocessed_{suffix}.fif\"\n",
    "    run_files = sorted(session_dir.glob(pattern))\n",
    "    if not run_files:\n",
    "        raise FileNotFoundError(f\"No run files found for {subject} {session} at {session_dir}\")\n",
    "\n",
    "    raws = []\n",
    "    for path in run_files:\n",
    "        raw = mne.io.read_raw_fif(str(path), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        raws.append(raw)\n",
    "\n",
    "    if len(raws) == 1:\n",
    "        return raws[0]\n",
    "    return mne.concatenate_raws(raws, preload=True, verbose=False)\n",
    "\n",
    "\n",
    "def build_session_events(subject: str, session: str, sfreq: float) -> np.ndarray:\n",
    "    session_dir = pre_root / subject / session\n",
    "    run_files = sorted(session_dir.glob(f\"{subject}_{session}_run-*_preprocessed_after_rereferencing.fif\"))\n",
    "    if not run_files:\n",
    "        raise FileNotFoundError(f\"No after_rereferencing runs for {subject} {session}\")\n",
    "\n",
    "    events_list = []\n",
    "    sample_offset = 0\n",
    "    for path in run_files:\n",
    "        raw = mne.io.read_raw_fif(str(path), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        run_token = path.stem.split('run-')[-1].split('_')[0]\n",
    "        run_events = load_run_events(subject, session, f\"run-{run_token}\", raw.info['sfreq'])\n",
    "        if run_events.size > 0:\n",
    "            run_events[:, 0] += sample_offset\n",
    "            events_list.append(run_events)\n",
    "        sample_offset += raw.n_times\n",
    "    if not events_list:\n",
    "        return np.empty((0, 3), dtype=int)\n",
    "    events = np.vstack(events_list)\n",
    "    order = np.argsort(events[:, 0])\n",
    "    return events[order]\n",
    "\n",
    "\n",
    "def pick_roi(raw: mne.io.Raw, roi_label: str):\n",
    "    desired = requested_roi.get(roi_label, [])\n",
    "    picks = mne.pick_channels(raw.ch_names, include=desired)\n",
    "    used = list(desired)\n",
    "    if len(picks) == 0:\n",
    "        key = 'parietal' if 'Parieto' in roi_label else 'frontal'\n",
    "        fallback = config_roi.get(key, [])\n",
    "        picks = mne.pick_channels(raw.ch_names, include=fallback)\n",
    "        used = list(fallback)\n",
    "    return picks, used\n",
    "\n",
    "\n",
    "def compute_cohens_d(raw: mne.io.Raw, events: np.ndarray, roi_picks, mask):\n",
    "    event_id = {'familiar': 1, 'new': 2}\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=event_id,\n",
    "        tmin=-0.2,\n",
    "        tmax=0.6,\n",
    "        baseline=(-0.1, 0.0),\n",
    "        preload=True,\n",
    "        detrend=None,\n",
    "        event_repeated='drop',\n",
    "        verbose='ERROR'\n",
    "    )\n",
    "    fam = epochs['familiar'].get_data()[:, roi_picks, :].mean(axis=1)[:, mask]\n",
    "    new = epochs['new'].get_data()[:, roi_picks, :].mean(axis=1)[:, mask]\n",
    "    mean_diff = fam.mean(axis=0) - new.mean(axis=0)\n",
    "    std_fam = fam.std(axis=0, ddof=1)\n",
    "    std_new = new.std(axis=0, ddof=1)\n",
    "    n_fam = fam.shape[0]\n",
    "    n_new = new.shape[0]\n",
    "    pooled_sd = np.sqrt(((n_fam - 1) * std_fam**2 + (n_new - 1) * std_new**2) / max(n_fam + n_new - 2, 1))\n",
    "    pooled_sd = np.where(pooled_sd == 0, np.nan, pooled_sd)\n",
    "    d_vals = mean_diff / pooled_sd\n",
    "    return d_vals\n",
    "\n",
    "cohens_records = []\n",
    "subjects_for_d = sorted(set(summary_df['subject']) | {manual_subject})\n",
    "for subject in subjects_for_d:\n",
    "    for session in sorted(summary_df[summary_df['subject'] == subject]['session'].unique()):\n",
    "        try:\n",
    "            raw_before = merge_stage(subject, session, pre_root, 'after_rereferencing')\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        raw_before.load_data()\n",
    "        events = build_session_events(subject, session, raw_before.info['sfreq'])\n",
    "        if events.size == 0:\n",
    "            continue\n",
    "        try:\n",
    "            raw_after = merge_stage(subject, session, after_root, 'ica_cleaned')\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        raw_after.load_data()\n",
    "\n",
    "        times = raw_before.times\n",
    "        mask = times >= 0\n",
    "        times_ms = (times[mask] * 1000).round(1)\n",
    "\n",
    "        for stage_label, raw_stage in [('Before ICA', raw_before), ('After ICA', raw_after)]:\n",
    "            for roi_label in requested_roi:\n",
    "                picks, used_channels = pick_roi(raw_stage, roi_label)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    d_vals = compute_cohens_d(raw_stage, events, picks, mask)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                for t, d in zip(times_ms, d_vals):\n",
    "                    cohens_records.append(\n",
    "                        {\n",
    "                            'subject': subject,\n",
    "                            'session': session,\n",
    "                            'Stage': stage_label,\n",
    "                            'ROI': roi_label,\n",
    "                            'Channels Used': ','.join(used_channels),\n",
    "                            'Time (ms)': float(t),\n",
    "                            \"Cohen's d\": float(d),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "cohens_df = pd.DataFrame(cohens_records)\n",
    "if cohens_df.empty:\n",
    "    raise RuntimeError('Cohen’s d computation produced no results.')\n",
    "\n",
    "cohens_output = RESULTS_DIR / 'cohens_d_summary.csv'\n",
    "cohens_df.to_csv(cohens_output, index=False)\n",
    "print(f\"Saved Cohen's d timecourse summary → {cohens_output}\")\n",
    "\n",
    "idx = cohens_df.groupby(['subject', 'session', 'Stage', 'ROI'])['Cohen‘s d'].apply(lambda s: s.abs().idxmax())\n",
    "peak_df = cohens_df.loc[idx]\n",
    "peak_output = RESULTS_DIR / 'cohens_d_peaks.csv'\n",
    "peak_df.to_csv(peak_output, index=False)\n",
    "print(f\"Saved Cohen's d peak summary → {peak_output}\")\n",
    "peak_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m pre_root \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_rereferencing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m after_root \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_ica\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m summary_session_counts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 23\u001b[0m     \u001b[43msummary_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_runs\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_run_events\u001b[39m(subject: \u001b[38;5;28mstr\u001b[39m, session: \u001b[38;5;28mstr\u001b[39m, run_token: \u001b[38;5;28mstr\u001b[39m, sfreq: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     28\u001b[0m     events_file \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m         raw_events_root\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;241m/\u001b[39m subject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_task-gonogo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_events.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_df' is not defined"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "requested_roi = {\n",
    "    'Frontal ROI': ['FP1', 'FP2'],\n",
    "    'Parietal ROI': ['P3', 'P3\"', 'P4', 'P4\"', 'PZ', 'PZ\"', 'CZ'],\n",
    "}\n",
    "config_roi = config['erp_analysis']['roi']\n",
    "\n",
    "familiar_labels = {\n",
    "    'animal_target', 'nonanimal_target', 'easy_target', 'difficult_target'\n",
    "}\n",
    "new_labels = {\n",
    "    'animal_distractor', 'nonanimal_distractor', 'easy_distractor', 'difficult_distractor'\n",
    "}\n",
    "valid_labels = familiar_labels | new_labels\n",
    "raw_events_root = project_root / 'ds002680'\n",
    "pre_root = project_root / 'data' / 'preprocessed' / 'after_rereferencing'\n",
    "after_root = project_root / 'data' / 'preprocessed' / 'after_ica'\n",
    "\n",
    "summary_session_counts = (\n",
    "    summary_df.groupby(['subject', 'session']).size().rename('n_runs').to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def load_run_events(subject: str, session: str, run_token: str, sfreq: float) -> np.ndarray:\n",
    "    events_file = (\n",
    "        raw_events_root\n",
    "        / subject\n",
    "        / session\n",
    "        / 'eeg'\n",
    "        / f\"{subject}_{session}_task-gonogo_{run_token}_events.tsv\"\n",
    "    )\n",
    "    if not events_file.exists():\n",
    "        raise FileNotFoundError(f\"Events TSV missing: {events_file}\")\n",
    "    df = pd.read_csv(events_file, sep='\t')\n",
    "    stim_df = df[df['value'].isin(valid_labels)].copy()\n",
    "    if stim_df.empty:\n",
    "        return np.empty((0, 3), dtype=int)\n",
    "    samples = (stim_df['onset'].values * sfreq).round().astype(int)\n",
    "    codes = np.array([1 if val in familiar_labels else 2 for val in stim_df['value']], dtype=int)\n",
    "    events = np.column_stack([samples, np.zeros(len(samples), dtype=int), codes])\n",
    "    return events\n",
    "\n",
    "\n",
    "def merge_stage(subject: str, session: str, stage_root: Path, suffix: str) -> mne.io.Raw:\n",
    "    session_dir = stage_root / subject / session\n",
    "    if not session_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing stage directory: {session_dir}\")\n",
    "\n",
    "    candidates = sorted(session_dir.glob(f\"{subject}_{session}_preprocessed_ica*_cleaned.fif\"))\n",
    "    if suffix == 'ica_cleaned' and candidates:\n",
    "        raw = mne.io.read_raw_fif(str(candidates[0]), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        return raw\n",
    "\n",
    "    pattern = f\"{subject}_{session}_run-*_preprocessed_{suffix}.fif\"\n",
    "    run_files = sorted(session_dir.glob(pattern))\n",
    "    if not run_files:\n",
    "        raise FileNotFoundError(f\"No run files found for {subject} {session} at {session_dir}\")\n",
    "\n",
    "    raws = []\n",
    "    for path in run_files:\n",
    "        raw = mne.io.read_raw_fif(str(path), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        raws.append(raw)\n",
    "\n",
    "    if len(raws) == 1:\n",
    "        return raws[0]\n",
    "    return mne.concatenate_raws(raws, preload=True, verbose=False)\n",
    "\n",
    "\n",
    "def build_session_events(subject: str, session: str, sfreq: float) -> np.ndarray:\n",
    "    session_dir = pre_root / subject / session\n",
    "    run_files = sorted(session_dir.glob(f\"{subject}_{session}_run-*_preprocessed_after_rereferencing.fif\"))\n",
    "    if not run_files:\n",
    "        raise FileNotFoundError(f\"No after_rereferencing runs for {subject} {session}\")\n",
    "\n",
    "    events_list = []\n",
    "    sample_offset = 0\n",
    "    for path in run_files:\n",
    "        raw = mne.io.read_raw_fif(str(path), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        run_token = path.stem.split('run-')[-1].split('_')[0]\n",
    "        run_events = load_run_events(subject, session, f\"run-{run_token}\", raw.info['sfreq'])\n",
    "        if run_events.size > 0:\n",
    "            run_events[:, 0] += sample_offset\n",
    "            events_list.append(run_events)\n",
    "        sample_offset += raw.n_times\n",
    "    if not events_list:\n",
    "        return np.empty((0, 3), dtype=int)\n",
    "    events = np.vstack(events_list)\n",
    "    order = np.argsort(events[:, 0])\n",
    "    return events[order]\n",
    "\n",
    "\n",
    "def pick_roi(raw: mne.io.Raw, roi_label: str):\n",
    "    desired = requested_roi.get(roi_label, [])\n",
    "    existing = [ch for ch in desired if ch in raw.ch_names]\n",
    "    if existing:\n",
    "        picks = mne.pick_channels(raw.ch_names, existing, ordered=True)\n",
    "        return picks, existing\n",
    "    key = 'parietal' if 'Parieto' in roi_label else 'frontal'\n",
    "    fallback = [ch for ch in config_roi.get(key, []) if ch in raw.ch_names]\n",
    "    if fallback:\n",
    "        picks = mne.pick_channels(raw.ch_names, fallback, ordered=True)\n",
    "        return picks, fallback\n",
    "    return np.array([], dtype=int), []\n",
    "\n",
    "\n",
    "def compute_cohens_d(raw: mne.io.Raw, events: np.ndarray, roi_picks, mask):\n",
    "    event_id = {'familiar': 1, 'new': 2}\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=event_id,\n",
    "        tmin=-0.2,\n",
    "        tmax=0.6,\n",
    "        baseline=(-0.1, 0.0),\n",
    "        preload=True,\n",
    "        detrend=None,\n",
    "        event_repeated='drop',\n",
    "        verbose='ERROR'\n",
    "    )\n",
    "    fam = epochs['familiar'].get_data()[:, roi_picks, :].mean(axis=1)[:, mask]\n",
    "    new = epochs['new'].get_data()[:, roi_picks, :].mean(axis=1)[:, mask]\n",
    "    mean_diff = fam.mean(axis=0) - new.mean(axis=0)\n",
    "    std_fam = fam.std(axis=0, ddof=1)\n",
    "    std_new = new.std(axis=0, ddof=1)\n",
    "    n_fam = fam.shape[0]\n",
    "    n_new = new.shape[0]\n",
    "    pooled_sd = np.sqrt(((n_fam - 1) * std_fam**2 + (n_new - 1) * std_new**2) / max(n_fam + n_new - 2, 1))\n",
    "    pooled_sd = np.where(pooled_sd == 0, np.nan, pooled_sd)\n",
    "    d_vals = mean_diff / pooled_sd\n",
    "    return d_vals\n",
    "\n",
    "cohens_records = []\n",
    "subjects_for_d = sorted(set(summary_df['subject']) | {manual_subject})\n",
    "for subject in subjects_for_d:\n",
    "    subject_sessions = summary_df.loc[summary_df['subject'] == subject, 'session'].unique()\n",
    "    for session in sorted(subject_sessions):\n",
    "        try:\n",
    "            raw_before = merge_stage(subject, session, pre_root, 'after_rereferencing')\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        raw_before.load_data()\n",
    "        events = build_session_events(subject, session, raw_before.info['sfreq'])\n",
    "        if events.size == 0:\n",
    "            continue\n",
    "        try:\n",
    "            raw_after = merge_stage(subject, session, after_root, 'ica_cleaned')\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        raw_after.load_data()\n",
    "\n",
    "        times = raw_before.times\n",
    "        mask = times >= 0\n",
    "        times_ms = (times[mask] * 1000).round(1)\n",
    "\n",
    "        for stage_label, raw_stage in [('Before ICA', raw_before), ('After ICA', raw_after)]:\n",
    "            for roi_label in requested_roi:\n",
    "                picks, used_channels = pick_roi(raw_stage, roi_label)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    d_vals = compute_cohens_d(raw_stage, events, picks, mask)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                for t, d in zip(times_ms, d_vals):\n",
    "                    cohens_records.append(\n",
    "                        {\n",
    "                            'subject': subject,\n",
    "                            'session': session,\n",
    "                            'Stage': stage_label,\n",
    "                            'ROI': roi_label,\n",
    "                            'Channels Used': ','.join(used_channels),\n",
    "                            'Time (ms)': float(t),\n",
    "                            \"Cohen's d\": float(d),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "cohens_df = pd.DataFrame(cohens_records)\n",
    "if cohens_df.empty:\n",
    "    raise RuntimeError(\"Cohen's d computation produced no results.\")\n",
    "\n",
    "cohens_output = RESULTS_DIR / 'cohens_d_summary.csv'\n",
    "cohens_df.to_csv(cohens_output, index=False)\n",
    "print(f\"Saved Cohen's d timecourse summary → {cohens_output}\")\n",
    "\n",
    "idx = cohens_df.groupby(['subject', 'session', 'Stage', 'ROI'])['Cohen‘s d'].apply(lambda s: s.abs().idxmax())\n",
    "peak_df = cohens_df.loc[idx]\n",
    "peak_output = RESULTS_DIR / 'cohens_d_peaks.csv'\n",
    "peak_df.to_csv(peak_output, index=False)\n",
    "print(f\"Saved Cohen's d peak summary → {peak_output}\")\n",
    "peak_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m pre_root \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_rereferencing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m after_root \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_ica\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m summary_session_counts \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 23\u001b[0m     \u001b[43msummary_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_runs\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_run_events\u001b[39m(subject: \u001b[38;5;28mstr\u001b[39m, session: \u001b[38;5;28mstr\u001b[39m, run_token: \u001b[38;5;28mstr\u001b[39m, sfreq: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     28\u001b[0m     events_file \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     29\u001b[0m         raw_events_root\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;241m/\u001b[39m subject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_task-gonogo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_events.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_df' is not defined"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "requested_roi = {\n",
    "    'Frontal ROI': ['FP1', 'FP2'],\n",
    "    'Parietal ROI': ['P3', 'P3\"', 'P4', 'P4\"', 'PZ', 'PZ\"', 'CZ'],\n",
    "}\n",
    "config_roi = config['erp_analysis']['roi']\n",
    "\n",
    "familiar_labels = {\n",
    "    'animal_target', 'nonanimal_target', 'easy_target', 'difficult_target'\n",
    "}\n",
    "new_labels = {\n",
    "    'animal_distractor', 'nonanimal_distractor', 'easy_distractor', 'difficult_distractor'\n",
    "}\n",
    "valid_labels = familiar_labels | new_labels\n",
    "raw_events_root = project_root / 'ds002680'\n",
    "pre_root = project_root / 'data' / 'preprocessed' / 'after_rereferencing'\n",
    "after_root = project_root / 'data' / 'preprocessed' / 'after_ica'\n",
    "\n",
    "summary_session_counts = (\n",
    "    summary_df.groupby(['subject', 'session']).size().rename('n_runs').to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def load_run_events(subject: str, session: str, run_token: str, sfreq: float) -> np.ndarray:\n",
    "    events_file = (\n",
    "        raw_events_root\n",
    "        / subject\n",
    "        / session\n",
    "        / 'eeg'\n",
    "        / f\"{subject}_{session}_task-gonogo_{run_token}_events.tsv\"\n",
    "    )\n",
    "    if not events_file.exists():\n",
    "        raise FileNotFoundError(f\"Events TSV missing: {events_file}\")\n",
    "    df = pd.read_csv(events_file, sep='\t')\n",
    "    stim_df = df[df['value'].isin(valid_labels)].copy()\n",
    "    if stim_df.empty:\n",
    "        return np.empty((0, 3), dtype=int)\n",
    "    samples = (stim_df['onset'].values * sfreq).round().astype(int)\n",
    "    codes = np.array([1 if val in familiar_labels else 2 for val in stim_df['value']], dtype=int)\n",
    "    events = np.column_stack([samples, np.zeros(len(samples), dtype=int), codes])\n",
    "    return events\n",
    "\n",
    "\n",
    "def merge_stage(subject: str, session: str, stage_root: Path, suffix: str) -> mne.io.Raw:\n",
    "    session_dir = stage_root / subject / session\n",
    "    if not session_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing stage directory: {session_dir}\")\n",
    "\n",
    "    candidates = sorted(session_dir.glob(f\"{subject}_{session}_preprocessed_ica*_cleaned.fif\"))\n",
    "    if suffix == 'ica_cleaned' and candidates:\n",
    "        raw = mne.io.read_raw_fif(str(candidates[0]), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        return raw\n",
    "\n",
    "    pattern = f\"{subject}_{session}_run-*_preprocessed_{suffix}.fif\"\n",
    "    run_files = sorted(session_dir.glob(pattern))\n",
    "    if not run_files:\n",
    "        raise FileNotFoundError(f\"No run files found for {subject} {session} at {session_dir}\")\n",
    "\n",
    "    raws = []\n",
    "    for path in run_files:\n",
    "        raw = mne.io.read_raw_fif(str(path), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        raws.append(raw)\n",
    "\n",
    "    if len(raws) == 1:\n",
    "        return raws[0]\n",
    "    return mne.concatenate_raws(raws, preload=True, verbose=False)\n",
    "\n",
    "\n",
    "def build_session_events(subject: str, session: str, sfreq: float) -> np.ndarray:\n",
    "    session_dir = pre_root / subject / session\n",
    "    run_files = sorted(session_dir.glob(f\"{subject}_{session}_run-*_preprocessed_after_rereferencing.fif\"))\n",
    "    if not run_files:\n",
    "        raise FileNotFoundError(f\"No after_rereferencing runs for {subject} {session}\")\n",
    "\n",
    "    events_list = []\n",
    "    sample_offset = 0\n",
    "    for path in run_files:\n",
    "        raw = mne.io.read_raw_fif(str(path), preload=True, verbose='ERROR')\n",
    "        raw.load_data()\n",
    "        run_token = path.stem.split('run-')[-1].split('_')[0]\n",
    "        run_events = load_run_events(subject, session, f\"run-{run_token}\", raw.info['sfreq'])\n",
    "        if run_events.size > 0:\n",
    "            run_events[:, 0] += sample_offset\n",
    "            events_list.append(run_events)\n",
    "        sample_offset += raw.n_times\n",
    "    if not events_list:\n",
    "        return np.empty((0, 3), dtype=int)\n",
    "    events = np.vstack(events_list)\n",
    "    order = np.argsort(events[:, 0])\n",
    "    return events[order]\n",
    "\n",
    "\n",
    "def pick_roi(raw: mne.io.Raw, roi_label: str):\n",
    "    desired = requested_roi.get(roi_label, [])\n",
    "    existing = [ch for ch in desired if ch in raw.ch_names]\n",
    "    if existing:\n",
    "        picks = mne.pick_channels(raw.ch_names, existing, ordered=True)\n",
    "        return picks, existing\n",
    "    key = 'parietal' if 'Parieto' in roi_label else 'frontal'\n",
    "    fallback = [ch for ch in config_roi.get(key, []) if ch in raw.ch_names]\n",
    "    if fallback:\n",
    "        picks = mne.pick_channels(raw.ch_names, fallback, ordered=True)\n",
    "        return picks, fallback\n",
    "    return np.array([], dtype=int), []\n",
    "\n",
    "\n",
    "def compute_cohens_d(raw: mne.io.Raw, events: np.ndarray, roi_picks, mask):\n",
    "    event_id = {'familiar': 1, 'new': 2}\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id=event_id,\n",
    "        tmin=-0.2,\n",
    "        tmax=0.6,\n",
    "        baseline=(-0.2, 0.0),\n",
    "        preload=True,\n",
    "        detrend=None,\n",
    "        event_repeated='drop',\n",
    "        verbose='ERROR'\n",
    "    )\n",
    "    fam = epochs['familiar'].get_data()[:, roi_picks, :].mean(axis=1)[:, mask]\n",
    "    new = epochs['new'].get_data()[:, roi_picks, :].mean(axis=1)[:, mask]\n",
    "    mean_diff = fam.mean(axis=0) - new.mean(axis=0)\n",
    "    std_fam = fam.std(axis=0, ddof=1)\n",
    "    std_new = new.std(axis=0, ddof=1)\n",
    "    n_fam = fam.shape[0]\n",
    "    n_new = new.shape[0]\n",
    "    pooled_sd = np.sqrt(((n_fam - 1) * std_fam**2 + (n_new - 1) * std_new**2) / max(n_fam + n_new - 2, 1))\n",
    "    pooled_sd = np.where(pooled_sd == 0, np.nan, pooled_sd)\n",
    "    d_vals = mean_diff / pooled_sd\n",
    "    return d_vals\n",
    "\n",
    "cohens_records = []\n",
    "subjects_for_d = sorted(set(summary_df['subject']) | {manual_subject})\n",
    "for subject in subjects_for_d:\n",
    "    subject_sessions = summary_df.loc[summary_df['subject'] == subject, 'session'].unique()\n",
    "    for session in sorted(subject_sessions):\n",
    "        try:\n",
    "            raw_before = merge_stage(subject, session, pre_root, 'after_rereferencing')\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        raw_before.load_data()\n",
    "        events = build_session_events(subject, session, raw_before.info['sfreq'])\n",
    "        if events.size == 0:\n",
    "            continue\n",
    "        try:\n",
    "            raw_after = merge_stage(subject, session, after_root, 'ica_cleaned')\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        raw_after.load_data()\n",
    "\n",
    "        times = raw_before.times\n",
    "        mask = times >= 0\n",
    "        times_ms = (times[mask] * 1000).round(1)\n",
    "\n",
    "        for stage_label, raw_stage in [('Before ICA', raw_before), ('After ICA', raw_after)]:\n",
    "            for roi_label in requested_roi:\n",
    "                picks, used_channels = pick_roi(raw_stage, roi_label)\n",
    "                if len(picks) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    d_vals = compute_cohens_d(raw_stage, events, picks, mask)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                for t, d in zip(times_ms, d_vals):\n",
    "                    cohens_records.append(\n",
    "                        {\n",
    "                            'subject': subject,\n",
    "                            'session': session,\n",
    "                            'Stage': stage_label,\n",
    "                            'ROI': roi_label,\n",
    "                            'Channels Used': ','.join(used_channels),\n",
    "                            'Time (ms)': float(t),\n",
    "                            \"Cohen's d\": float(d),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "cohens_df = pd.DataFrame(cohens_records)\n",
    "if cohens_df.empty:\n",
    "    raise RuntimeError(\"Cohen's d computation produced no results.\")\n",
    "\n",
    "cohens_output = RESULTS_DIR / 'cohens_d_summary.csv'\n",
    "cohens_df.to_csv(cohens_output, index=False)\n",
    "print(f\"Saved Cohen's d timecourse summary → {cohens_output}\")\n",
    "\n",
    "idx = cohens_df.groupby(['subject', 'session', 'Stage', 'ROI'])['Cohen‘s d'].apply(lambda s: s.abs().idxmax())\n",
    "peak_df = cohens_df.loc[idx]\n",
    "peak_output = RESULTS_DIR / 'cohens_d_peaks.csv'\n",
    "peak_df.to_csv(peak_output, index=False)\n",
    "print(f\"Saved Cohen's d peak summary → {peak_output}\")\n",
    "peak_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
