{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15c5d75",
   "metadata": {},
   "source": [
    "# EEG Memory Recognition Analysis - ERP Analysis\n",
    "\n",
    "This notebook implements epoching, artifact rejection, and Event-Related Potential (ERP) analysis.\n",
    "\n",
    "## Analysis Goals\n",
    "Based on Delorme et al. (2018), we analyze:\n",
    "\n",
    "1. **Familiarity Effect**: ERP differences between familiar vs. novel images\n",
    "2. **Repetition Effect**: How recognition ERPs change across repeated exposures\n",
    "3. **Category Effect**: Animal vs. non-animal stimulus processing differences\n",
    "\n",
    "## ROI Analysis\n",
    "- **Frontal ROI**: F3, FZ, F4 - Early recognition components (N200, P300)\n",
    "- **Parietal ROI**: P3, PZ, P4 - Late recognition components (P600, LPC)\n",
    "\n",
    "## Statistical Analysis\n",
    "- T-tests with FDR correction for multiple comparisons\n",
    "- Time-window analysis for significant effects\n",
    "- Visualization matching the original study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54c5bf",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f909d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "Project root: /Users/leeyelim/Documents/EEG\n",
      "‚úì Setup complete\n",
      "MNE version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = (notebook_dir / \"..\").resolve()\n",
    "src_dir = project_root / \"src\"\n",
    "\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "try:\n",
    "    from utils.pathing import ensure_src_on_path, project_paths\n",
    "    ensure_src_on_path()\n",
    "    from utils.data_loader import EEGDataLoader\n",
    "    from preprocessing.quality_assessment import EEGQualityAssessment\n",
    "    print(\"‚úÖ Imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import note: {e}\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‚úì Setup complete\")\n",
    "print(f\"MNE version: {mne.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebbf6f",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376f6689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:43:00,847 - INFO - EEGDataLoader initialized\n",
      "  Project root: /Users/leeyelim/Documents/EEG\n",
      "  Config: /Users/leeyelim/Documents/EEG/config/analysis_config.yaml\n",
      "  Raw dir: /Users/leeyelim/Documents/EEG/ds002680 (exists=True)\n",
      "  Preprocessed dir: /Users/leeyelim/Documents/EEG/data/preprocessed (exists=True)\n",
      "  Derivatives dir: /Users/leeyelim/Documents/EEG/data/derivatives (exists=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded!\n",
      "- Selected subjects: 10\n",
      "- Epoching: -0.1 to 0.6 s\n",
      "- ROI channels: {'frontal': ['FP1', 'FP2'], 'parieto_occipital': ['P3', 'P3\"', 'P4', 'P4\"', 'PZ', 'PZ\"', 'CZ']}\n",
      "\n",
      "‚úì Data loader initialized\n"
     ]
    }
   ],
   "source": [
    "config_path = project_root / 'config' / 'analysis_config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "selected_subjects = config['subjects']['selected']\n",
    "manual_ica_subject = config['subjects']['manual_ica_subject']\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"- Selected subjects: {len(selected_subjects)}\")\n",
    "print(f\"- Epoching: {config['preprocessing']['epoching']['tmin']} to {config['preprocessing']['epoching']['tmax']} s\")\n",
    "print(f\"- ROI channels: {config['erp_analysis']['roi']}\")\n",
    "\n",
    "data_loader = EEGDataLoader(config_path=str(config_path))\n",
    "print(\"\\n‚úì Data loader initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8f873",
   "metadata": {},
   "source": [
    "## 3. Load ICA-Cleaned Data\n",
    "\n",
    "Load the ICA-cleaned data for the manual subject to demonstrate ERP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8822734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Loading ICA-cleaned data for sub-003...\n",
      "Loading: /Users/leeyelim/Documents/EEG/data/preprocessed/after_ica/sub-003/ses-02/sub-003_ses-02_run-3_preprocessed_ica_cleaned.fif\n",
      "‚úÖ Loaded: 31 channels, 209.3s\n",
      "Session: ses-02, Run: preprocessed\n"
     ]
    }
   ],
   "source": [
    "# Load ICA-cleaned data\n",
    "print(f\"\\nüî¨ Loading ICA-cleaned data for {manual_ica_subject}...\")\n",
    "\n",
    "ica_dir = project_root / 'data' / 'preprocessed' / 'after_ica'\n",
    "subject_dir = ica_dir / manual_ica_subject\n",
    "\n",
    "if subject_dir.exists():\n",
    "    fif_files = list(subject_dir.rglob('*ica_cleaned.fif'))\n",
    "    if fif_files:\n",
    "        raw_file = fif_files[0]\n",
    "        print(f\"Loading: {raw_file}\")\n",
    "        raw = mne.io.read_raw_fif(str(raw_file), preload=True, verbose=False)\n",
    "        print(f\"‚úÖ Loaded: {raw.info['nchan']} channels, {raw.times[-1]:.1f}s\")\n",
    "        \n",
    "        # Get session and run info from filename\n",
    "        parts = raw_file.stem.split('_')\n",
    "        session = parts[1]\n",
    "        run = parts[3]\n",
    "        print(f\"Session: {session}, Run: {run}\")\n",
    "    else:\n",
    "        print(\"‚ùå No ICA-cleaned files found\")\n",
    "        print(\"Please run 02_manual_ica_review.ipynb first\")\n",
    "        raw = None\n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {subject_dir}\")\n",
    "    print(\"Please run 01 and 02 notebooks first\")    raw = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c20aa6",
   "metadata": {},
   "source": [
    "## 4. Load Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3099fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ Loading events...\n",
      "‚ùå Error loading events: Could not find EEG file for sub-003 with session=ses-02 run=preprocessed. Found 25 candidate .set files under /Users/leeyelim/Documents/EEG/ds002680/sub-003. Examples: ['ses-02/eeg/sub-003_ses-02_task-gonogo_run-5_eeg.set', 'ses-02/eeg/sub-003_ses-02_task-gonogo_run-11_eeg.set', 'ses-02/eeg/sub-003_ses-02_task-gonogo_run-4_eeg.set', 'ses-02/eeg/sub-003_ses-02_task-gonogo_run-10_eeg.set', 'ses-02/eeg/sub-003_ses-02_task-gonogo_run-12_eeg.set']\n"
     ]
    }
   ],
   "source": [
    "if raw is not None:\n",
    "    # Load events for this session/run\n",
    "    print(\"\\nüìÖ Loading events...\")\n",
    "    try:\n",
    "        events, event_id = data_loader.load_events(\n",
    "            manual_ica_subject, \n",
    "            session=session, \n",
    "            run=run, \n",
    "            task='gonogo'\n",
    "        )\n",
    "        \n",
    "        if events is not None:\n",
    "            print(f\"‚úÖ Loaded {len(events)} events\")\n",
    "            print(f\"Event types: {event_id}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No events found\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading events: {e}\")\n",
    "        events = None\n",
    "        event_id = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping: No data loaded\")\n",
    "    events = None\n",
    "    event_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1caf117",
   "metadata": {},
   "source": [
    "## 5. Create Epochs\n",
    "\n",
    "Epoch the data from -100 to 600 ms around stimulus onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2385154",
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw is not None and events is not None:\n",
    "    # Create epochs\n",
    "    print(\"\\nüîÑ Creating epochs...\")\n",
    "    \n",
    "    tmin = config['preprocessing']['epoching']['tmin']\n",
    "    tmax = config['preprocessing']['epoching']['tmax']\n",
    "    baseline = tuple(config['preprocessing']['epoching']['baseline'])\n",
    "    \n",
    "    epochs = mne.Epochs(\n",
    "        raw, events, event_id=event_id,\n",
    "        tmin=tmin, tmax=tmax,\n",
    "        baseline=baseline,\n",
    "        preload=True,\n",
    "        reject=dict(eeg=100e-6),  # Reject epochs with amplitude > 100 ¬µV\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(epochs)} epochs\")\n",
    "    print(f\"  Time range: {tmin}s to {tmax}s\")\n",
    "    print(f\"  Baseline: {baseline}\")\n",
    "    \n",
    "    # Display event counts\n",
    "    print(\"\\nüìä Epochs per condition:\")\n",
    "    for condition in epochs.event_id.keys():\n",
    "        n_epochs = len(epochs[condition])\n",
    "        print(f\"  {condition}: {n_epochs} epochs\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping: No data or events loaded\")\n",
    "    epochs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e9a5b",
   "metadata": {},
   "source": [
    "## 6. Compute ERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epochs is not None:\n",
    "    # Compute ERPs (evoked responses) for each condition\n",
    "    print(\"\\nüîÑ Computing ERPs...\")\n",
    "    \n",
    "    erps = {}\n",
    "    for condition in epochs.event_id.keys():\n",
    "        if condition in epochs:\n",
    "            erp = epochs[condition].average()\n",
    "            erps[condition] = erp\n",
    "            print(f\"  ‚úÖ {condition}: {len(epochs[condition])} trials averaged\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Computed ERPs for {len(erps)} conditions\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping: No epochs available\")\n",
    "    erps = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e2e79",
   "metadata": {},
   "source": [
    "## 7. Visualize ERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if erps:\n",
    "    # Plot ERPs for all conditions\n",
    "    print(\"\\nüìä Visualizing ERPs...\")\n",
    "    \n",
    "    # Get ROI channels\n",
    "    roi_channels = config['erp_analysis']['roi']\n",
    "    frontal_chs = roi_channels['frontal']\n",
    "    parietal_chs = roi_channels['parieto_occipital']\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(f'Event-Related Potentials - {manual_ica_subject}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: All conditions, frontal ROI\n",
    "    ax = axes[0, 0]\n",
    "    times = erps[list(erps.keys())[0]].times\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for i, (condition, erp) in enumerate(erps.items()):\n",
    "        # Pick frontal channels\n",
    "        try:\n",
    "            frontal_data = erp.copy().pick_channels(frontal_chs).get_data()\n",
    "            frontal_mean = np.mean(frontal_data, axis=0) * 1e6  # Convert to ¬µV\n",
    "            ax.plot(times, frontal_mean, color=colors[i % len(colors)], \n",
    "                   linewidth=2, label=condition)\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è Could not plot {condition} for frontal ROI\")\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude (¬µV)')\n",
    "    ax.set_title('Frontal ROI')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='Stimulus')\n",
    "    \n",
    "    # Plot 2: All conditions, parietal ROI\n",
    "    ax = axes[0, 1]\n",
    "    for i, (condition, erp) in enumerate(erps.items()):\n",
    "        try:\n",
    "            parietal_data = erp.copy().pick_channels(parietal_chs).get_data()\n",
    "            parietal_mean = np.mean(parietal_data, axis=0) * 1e6\n",
    "            ax.plot(times, parietal_mean, color=colors[i % len(colors)], \n",
    "                   linewidth=2, label=condition)\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è Could not plot {condition} for parietal ROI\")\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude (¬µV)')\n",
    "    ax.set_title('Parietal ROI')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 3: Topographic map at peak (example)\n",
    "    ax = axes[1, 0]\n",
    "    if len(erps) > 0:\n",
    "        first_erp = list(erps.values())[0]\n",
    "        peak_time = 0.3  # Example: 300ms\n",
    "        try:\n",
    "            first_erp.plot_topomap(times=[peak_time], axes=ax, show=False, \n",
    "                                  colorbar=True, time_format='%0.3f s')\n",
    "            ax.set_title(f'Topography at {peak_time}s')\n",
    "        except:\n",
    "            ax.text(0.5, 0.5, 'Topography visualization\\nrequires standard montage', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Plot 4: Difference wave (if applicable)\n",
    "    ax = axes[1, 1]\n",
    "    # Example: Familiar - New difference\n",
    "    familiar_conds = [k for k in erps.keys() if 'familiar' in k.lower()]\n",
    "    new_conds = [k for k in erps.keys() if 'new' in k.lower()]\n",
    "    \n",
    "    if familiar_conds and new_conds:\n",
    "        try:\n",
    "            fam_data = erps[familiar_conds[0]].copy().pick_channels(parietal_chs).get_data()\n",
    "            new_data = erps[new_conds[0]].copy().pick_channels(parietal_chs).get_data()\n",
    "            diff = (np.mean(fam_data, axis=0) - np.mean(new_data, axis=0)) * 1e6\n",
    "            ax.plot(times, diff, 'purple', linewidth=2, label='Familiar - New')\n",
    "            ax.fill_between(times, 0, diff, alpha=0.3, color='purple')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude Difference (¬µV)')\n",
    "    ax.set_title('Difference Wave (Parietal ROI)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = project_root / 'results' / 'figures'\n",
    "    fig_path.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_path / f'erp_analysis_{manual_ica_subject}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüíæ ERP plot saved to: {fig_path / f'erp_analysis_{manual_ica_subject}.png'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping: No ERPs available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b6420",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis (Example)\n",
    "\n",
    "Perform t-tests to compare conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epochs is not None and len(erps) >= 2:\n",
    "    print(\"\\nüìä Statistical Analysis...\")\n",
    "    \n",
    "    # Example: Compare familiar vs new\n",
    "    familiar_conds = [k for k in epochs.event_id.keys() if 'familiar' in k.lower()]\n",
    "    new_conds = [k for k in epochs.event_id.keys() if 'new' in k.lower()]\n",
    "    \n",
    "    if familiar_conds and new_conds:\n",
    "        try:\n",
    "            # Get data\n",
    "            fam_epochs = epochs[familiar_conds]\n",
    "            new_epochs = epochs[new_conds]\n",
    "            \n",
    "            # Pick parietal channels\n",
    "            fam_data = fam_epochs.copy().pick_channels(parietal_chs).get_data()\n",
    "            new_data = new_epochs.copy().pick_channels(parietal_chs).get_data()\n",
    "            \n",
    "            # Average across channels: (n_epochs, n_times)\n",
    "            fam_mean = np.mean(fam_data, axis=1)\n",
    "            new_mean = np.mean(new_data, axis=1)\n",
    "            \n",
    "            # T-test at each time point\n",
    "            t_stats = []\n",
    "            p_values = []\n",
    "            for t_idx in range(fam_mean.shape[1]):\n",
    "                t, p = stats.ttest_ind(fam_mean[:, t_idx], new_mean[:, t_idx])\n",
    "                t_stats.append(t)\n",
    "                p_values.append(p)\n",
    "            \n",
    "            # FDR correction\n",
    "            from statsmodels.stats.multitest import multipletests\n",
    "            _, p_corrected, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05)\n",
    "            \n",
    "            # Find significant time points\n",
    "            sig_times = np.where(p_corrected < 0.05)[0]\n",
    "            \n",
    "            print(f\"\\n‚úÖ Statistical comparison: {familiar_conds[0]} vs {new_conds[0]}\")\n",
    "            print(f\"  Significant timepoints: {len(sig_times)} / {len(p_values)}\")\n",
    "            if len(sig_times) > 0:\n",
    "                times_ms = epochs.times[sig_times] * 1000\n",
    "                print(f\"  Time range: {times_ms.min():.0f} - {times_ms.max():.0f} ms\")\n",
    "            else:\n",
    "                print(\"  No significant differences found (FDR corrected)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Statistical analysis note: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Need familiar and new conditions for comparison\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping: Insufficient data for statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f721e",
   "metadata": {},
   "source": [
    "## 9. Save Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744498d0",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epochs is not None:\n",
    "    # Save epochs for later analysis\n",
    "    epochs_dir = project_root / 'data' / 'preprocessed' / 'after_epochs'\n",
    "    subject_epochs_dir = epochs_dir / manual_ica_subject / session\n",
    "    subject_epochs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    epochs_filename = f\"{manual_ica_subject}_{session}_{run}_epo.fif\"\n",
    "    epochs_path = subject_epochs_dir / epochs_filename\n",
    "    \n",
    "    epochs.save(str(epochs_path), overwrite=True, verbose=False)\n",
    "    print(f\"\\nüíæ Epochs saved to: {epochs_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No epochs to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bf976",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "This demonstrates ERP analysis for one subject. To complete the analysis:\n",
    "\n",
    "1. **Loop through all subjects** to compute group-level ERPs\n",
    "2. **Implement full statistical pipeline** with:\n",
    "   - Repetition effects (1st, 2nd, 3rd presentation)\n",
    "   - Category effects (animal vs non-animal)\n",
    "   - Multiple comparison corrections\n",
    "3. **Create publication-quality figures** matching Delorme et al. (2018)\n",
    "4. **Create main_analysis.ipynb** for comprehensive results\n",
    "\n",
    "For production use, consider using the `src/analysis/erp_analysis.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ ERP ANALYSIS DEMONSTRATION COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ ERP analysis completed for {manual_ica_subject}\")\n",
    "print(\"\\nüìã NEXT STEPS:\")\n",
    "print(\"1. Extend to all subjects for group analysis\")\n",
    "print(\"2. Implement full statistical pipeline\")\n",
    "print(\"3. Create main_analysis.ipynb for final results\")\n",
    "print(\"\\nüìä Analysis Progress: 4/4 Complete (Demo)\")\n",
    "print(\"   ‚úì Setup and data exploration\")\n",
    "print(\"   ‚úì Preprocessing pipeline\")\n",
    "print(\"   ‚úì Manual ICA review\")\n",
    "print(\"   ‚úì ERP analysis (demonstration)\")\n",
    "print(\"   ‚Üí Next: Full group analysis + GitHub presentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f5d42",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa98b79",
   "metadata": {},
   "source": [
    "## 6. Group-level familiarity effect (familiar vs new) with FDR correction\n",
    "\n",
    "Uses aggregated ERP timecourses to run paired t-tests across subjects at each timepoint for each ROI, correcting p-values (BH-FDR). Saves CSVs under `results/statistical_outputs/` and plots group difference waves with significant intervals highlighted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65af866",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7420ee9",
   "metadata": {},
   "source": [
    "## 7. Repetition-wise familiarity difference (1st/2nd/3rd) per ROI\n",
    "\n",
    "Computes group-level Familiar ‚àí New difference ERPs separately for the 1st, 2nd, and 3rd repetition of the familiar set, and plots mean ¬± SEM for each repetition, matching the reference figure layout (Frontal ROI on the left, Parieto-occipital on the right).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91de23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21ceb3",
   "metadata": {},
   "source": [
    "### 7a. Repetition-wise repeated-measures ANOVA (per timepoint, per ROI)\n",
    "\n",
    "Computes one-way repeated-measures ANOVA across repetitions (1/2/3) at each timepoint for each ROI. Saves CSVs under `results/statistical_outputs/erp_rep_anova_<roi>.csv` with columns: `time_ms, F, p, p_fdr, n_subjects`. Uses BH-FDR within each ROI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f390e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd3055",
   "metadata": {},
   "source": [
    "### 7b. Save repetition-wise familiarity difference timecourses (mean ¬± SEM)\n",
    "\n",
    "Exports per-ROI repetition curves to CSV with columns: `time_ms, repetition, mean_uV, sem_uV, n`. Files are written to `results/statistical_outputs/erp_repetition_diff_<roi>.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf87c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aa420",
   "metadata": {},
   "source": [
    "## 8. Category analysis: Animal vs Non-animal\n",
    "\n",
    "This section reproduces category effects:\n",
    "- Channel √ó time p-value heatmap for Animal vs Non-animal (paired across subjects)\n",
    "- ROI panel of Familiar‚àíNew difference per category (Animal vs Non-animal), with mean ¬± SEM and timepoints where categories differ (BH-FDR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c3c89",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8695370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ea226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-subject category ERPs and channel√ótime stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "figdir = project_root / 'results' / 'figures'\n",
    "statdir = project_root / 'results' / 'statistical_outputs'\n",
    "figdir.mkdir(parents=True, exist_ok=True)\n",
    "statdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "roi_cfg = config['erp_analysis']['roi']\n",
    "FRONTAL = roi_cfg.get('frontal', [])\n",
    "PAROCC  = roi_cfg.get('parieto_occipital', [])\n",
    "\n",
    "familiar_labels = {'animal_target','nonanimal_target','easy_target','difficult_target'}\n",
    "new_labels      = {'animal_distractor','nonanimal_distractor','easy_distractor','difficult_distractor'}\n",
    "\n",
    "# helper to decide category label\n",
    "def _is_animal(v: str) -> bool:\n",
    "    return isinstance(v, str) and ('animal_' in v) and (not v.startswith('nonanimal'))\n",
    "\n",
    "def _is_nonanimal(v: str) -> bool:\n",
    "    return isinstance(v, str) and (v.startswith('nonanimal'))\n",
    "\n",
    "# Collect per-subject per-channel time series of category difference (fam‚àínew) and ROI series\n",
    "subj_chan_series = {}  # subj -> DataFrame (channels x times) of (animal‚àínonanimal) difference of (fam‚àínew)\n",
    "roi_series = {'Frontal ROI': {'animal': {}, 'nonanimal': {}},\n",
    "              'Parieto-occipital ROI': {'animal': {}, 'nonanimal': {}}}\n",
    "\n",
    "for subj in selected_subjects:\n",
    "    # aggregate across sessions\n",
    "    chan_acc_animal = []\n",
    "    chan_acc_nonanimal = []\n",
    "    times_ref = None\n",
    "    ch_names_ref = None\n",
    "    for ses in list_sessions(subj):\n",
    "        raw_after = merge_stage(subj, ses, after_ica_root, 'ica_cleaned')\n",
    "        if raw_after is None:\n",
    "            continue\n",
    "        evdf = build_session_events_with_meta(subj, ses)\n",
    "        if evdf is None or evdf.empty:\n",
    "            continue\n",
    "        tmin = config['preprocessing']['epoching']['tmin']\n",
    "        tmax = config['preprocessing']['epoching']['tmax']\n",
    "        baseline = tuple(config['preprocessing']['epoching']['baseline'])\n",
    "        # create events array for familiar/new with categories\n",
    "        ev_animal_fam = evdf[evdf['value'].apply(_is_animal) & evdf['value'].isin(familiar_labels)]['sample'].astype(int).values\n",
    "        ev_animal_new = evdf[evdf['value'].apply(_is_animal) & evdf['value'].isin(new_labels)]['sample'].astype(int).values\n",
    "        ev_non_fam = evdf[evdf['value'].apply(_is_nonanimal) & evdf['value'].isin(familiar_labels)]['sample'].astype(int).values\n",
    "        ev_non_new = evdf[evdf['value'].apply(_is_nonanimal) & evdf['value'].isin(new_labels)]['sample'].astype(int).values\n",
    "        def _epochs_from_samples(samples):\n",
    "            if samples.size == 0:\n",
    "                return None\n",
    "            arr = np.column_stack([samples, np.zeros(len(samples), dtype=int), np.ones(len(samples), dtype=int)])\n",
    "            try:\n",
    "                ep = mne.Epochs(raw_after, arr, event_id=None, tmin=tmin, tmax=tmax,\n",
    "                                baseline=baseline, preload=True, verbose='ERROR')\n",
    "                return ep\n",
    "            except Exception:\n",
    "                return None\n",
    "        ep_af = _epochs_from_samples(ev_animal_fam)\n",
    "        ep_an = _epochs_from_samples(ev_animal_new)\n",
    "        ep_nf = _epochs_from_samples(ev_non_fam)\n",
    "        ep_nn = _epochs_from_samples(ev_non_new)\n",
    "        if any(e is None or len(e)==0 for e in [ep_af, ep_an, ep_nf, ep_nn]):\n",
    "            continue\n",
    "        # channel-level evokeds\n",
    "        af = ep_af.average(); an = ep_an.average(); nf = ep_nf.average(); nn = ep_nn.average()\n",
    "        # fam‚àínew per category\n",
    "        animal_diff = (af.data - an.data) * 1e6  # channels x time (¬µV)\n",
    "        non_diff   = (nf.data - nn.data) * 1e6\n",
    "        chan_acc_animal.append(animal_diff)\n",
    "        chan_acc_nonanimal.append(non_diff)\n",
    "        times_ref = af.times * 1000.0\n",
    "        ch_names_ref = af.ch_names\n",
    "        # ROI series per session\n",
    "        for roi_name, picks in [('Frontal ROI', FRONTAL), ('Parieto-occipital ROI', PAROCC)]:\n",
    "            sel = [ch for ch in picks if ch in af.ch_names]\n",
    "            if not sel:\n",
    "                continue\n",
    "            idx = [af.ch_names.index(ch) for ch in sel]\n",
    "            roi_animal = animal_diff[idx, :].mean(axis=0)\n",
    "            roi_non    = non_diff[idx, :].mean(axis=0)\n",
    "            roi_series[roi_name]['animal'].setdefault(subj, []).append(pd.Series(roi_animal, index=times_ref))\n",
    "            roi_series[roi_name]['nonanimal'].setdefault(subj, []).append(pd.Series(roi_non, index=times_ref))\n",
    "    # average across sessions for subject at channel level\n",
    "    if times_ref is not None and ch_names_ref is not None and chan_acc_animal and chan_acc_nonanimal:\n",
    "        A = np.stack(chan_acc_animal, axis=0).mean(axis=0)\n",
    "        N = np.stack(chan_acc_nonanimal, axis=0).mean(axis=0)\n",
    "        subj_chan_series[subj] = {'animal': A, 'nonanimal': N, 'times_ms': times_ref, 'ch_names': ch_names_ref}\n",
    "\n",
    "# 1) Channel √ó time p-values heatmap (Animal vs Non-animal difference)\n",
    "if subj_chan_series:\n",
    "    ch_names = list(next(iter(subj_chan_series.values()))['ch_names'])\n",
    "    times_ms = next(iter(subj_chan_series.values()))['times_ms']\n",
    "    # stack subjects\n",
    "    animal_stack = []\n",
    "    non_stack = []\n",
    "    for subj, obj in subj_chan_series.items():\n",
    "        animal_stack.append(obj['animal'])\n",
    "        non_stack.append(obj['nonanimal'])\n",
    "    animal_stack = np.stack(animal_stack, axis=0)  # n x ch x t\n",
    "    non_stack    = np.stack(non_stack, axis=0)\n",
    "    # paired t-test across subjects for each ch,t\n",
    "    from scipy import stats as _stats\n",
    "    n_subj, n_ch, n_t = animal_stack.shape\n",
    "    p_mat = np.full((n_ch, n_t), np.nan)\n",
    "    for ci in range(n_ch):\n",
    "        x = animal_stack[:, ci, :]\n",
    "        y = non_stack[:, ci, :]\n",
    "        for ti in range(n_t):\n",
    "            xv = x[:, ti]; yv = y[:, ti]\n",
    "            mask = np.isfinite(xv) & np.isfinite(yv)\n",
    "            if mask.sum() < 2:\n",
    "                continue\n",
    "            t, p = _stats.ttest_rel(xv[mask], yv[mask])\n",
    "            p_mat[ci, ti] = p\n",
    "    # save CSV and figure\n",
    "    heat_df = pd.DataFrame(p_mat, index=ch_names, columns=np.round(times_ms,1))\n",
    "    heat_csv = statdir / 'erp_category_channel_time_pvals.csv'\n",
    "    heat_df.to_csv(heat_csv)\n",
    "    print(f\"üíæ Saved heatmap p-values ‚Üí {heat_csv}\")\n",
    "    plt.figure(figsize=(8,6))\n",
    "    # map p-values to -log10 for better dynamic range\n",
    "    with np.errstate(divide='ignore'):\n",
    "        z = -np.log10(p_mat)\n",
    "    sns.heatmap(z, cmap='inferno', yticklabels=ch_names, xticklabels=200,\n",
    "                cbar_kws={'label': '-log10(p)'} )\n",
    "    plt.title('Animal vs Non-animal (paired t-test across subjects): -log10(p)')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Channels')\n",
    "    heat_png = figdir / 'group_category_channel_time_pvals.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(heat_png, dpi=200, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved: {heat_png}\")\n",
    "    plt.show()\n",
    "\n",
    "# 2) ROI panel: category difference curves (Animal vs Non-animal) and FDR significance\n",
    "try:\n",
    "    _bh_fdr\n",
    "except NameError:\n",
    "    def _bh_fdr(pvals):\n",
    "        p = np.asarray(pvals, dtype=float); m = p.size\n",
    "        order = np.argsort(p); ranked = p[order]\n",
    "        adj = np.empty_like(ranked); prev=1.0\n",
    "        for i in range(m-1, -1, -1):\n",
    "            rank = i+1; val = ranked[i]*m/float(rank)\n",
    "            prev = min(prev, val) if np.isfinite(val) else prev\n",
    "            adj[i] = prev if np.isfinite(val) else np.nan\n",
    "        out = np.minimum(1.0, adj); res = np.empty_like(p); res[order]=out; return res\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5), sharey=True)\n",
    "for idx, (roi_name, ax) in enumerate([('Frontal ROI', axes[0]), ('Parieto-occipital ROI', axes[1])]):\n",
    "    # subject-averaged per category\n",
    "    subj_series_an = []\n",
    "    subj_series_non = []\n",
    "    common_index = None\n",
    "    for subj in sorted(set(roi_series[roi_name]['animal'].keys()) | set(roi_series[roi_name]['nonanimal'].keys())):\n",
    "        s_an = roi_series[roi_name]['animal'].get(subj)\n",
    "        s_non = roi_series[roi_name]['nonanimal'].get(subj)\n",
    "        if not s_an or not s_non:\n",
    "            continue\n",
    "        # average across sessions within subject\n",
    "        idx_union = sorted(set().union(*[s.index for s in (s_an+s_non)]))\n",
    "        an = np.nanmean(np.vstack([s.reindex(idx_union).values for s in s_an]), axis=0)\n",
    "        nn = np.nanmean(np.vstack([s.reindex(idx_union).values for s in s_non]), axis=0)\n",
    "        subj_series_an.append(pd.Series(an, index=idx_union))\n",
    "        subj_series_non.append(pd.Series(nn, index=idx_union))\n",
    "        common_index = idx_union\n",
    "    if not subj_series_an or not subj_series_non:\n",
    "        continue\n",
    "    A = np.vstack([s.reindex(common_index).values for s in subj_series_an])\n",
    "    N = np.vstack([s.reindex(common_index).values for s in subj_series_non])\n",
    "    meanA = np.nanmean(A, axis=0); semA = np.nanstd(A, axis=0, ddof=1)/np.sqrt(np.sum(np.isfinite(A), axis=0).clip(min=1))\n",
    "    meanN = np.nanmean(N, axis=0); semN = np.nanstd(N, axis=0, ddof=1)/np.sqrt(np.sum(np.isfinite(N), axis=0).clip(min=1))\n",
    "    # plot curves\n",
    "    ax.plot(common_index, meanA, color='magenta', lw=2, label='Animal')\n",
    "    ax.fill_between(common_index, meanA-semA, meanA+semA, color='magenta', alpha=0.25)\n",
    "    ax.plot(common_index, meanN, color='teal', lw=2, label='Non-animal')\n",
    "    ax.fill_between(common_index, meanN-semN, meanN+semN, color='teal', alpha=0.25)\n",
    "    # significance Animal vs Non-animal\n",
    "    from scipy import stats as _stats\n",
    "    pvals = []\n",
    "    for i in range(len(common_index)):\n",
    "        x = A[:, i]; y = N[:, i]\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        if mask.sum() < 2:\n",
    "            pvals.append(np.nan)\n",
    "        else:\n",
    "            t,p = _stats.ttest_rel(x[mask], y[mask])\n",
    "            pvals.append(p)\n",
    "    pvals = np.array(pvals)\n",
    "    valid = np.isfinite(pvals); p_fdr = np.full_like(pvals, np.nan)\n",
    "    if valid.any():\n",
    "        p_fdr[valid] = _bh_fdr(pvals[valid])\n",
    "    sig = (p_fdr < 0.05)\n",
    "    ymin, ymax = ax.get_ylim(); yspan = ymax - ymin; y_bar = ymin + 0.08*yspan\n",
    "    if np.any(sig):\n",
    "        t_ms = np.array(common_index)\n",
    "        step = np.median(np.diff(t_ms)) if len(t_ms)>1 else 1.0\n",
    "        start=None; prev=None\n",
    "        for tt in t_ms[sig]:\n",
    "            if start is None:\n",
    "                start=tt; prev=tt; continue\n",
    "            if abs(tt-prev-step)<1e-6:\n",
    "                prev=tt; continue\n",
    "            ax.hlines(y_bar, start, prev, colors='black', linewidth=4)\n",
    "            start=tt; prev=tt\n",
    "        if start is not None:\n",
    "            ax.hlines(y_bar, start, prev, colors='black', linewidth=4)\n",
    "    ax.axhline(0, color='k', lw=0.8, alpha=0.5)\n",
    "    ax.axvline(0, color='k', lw=1.0, ls='--', alpha=0.6)\n",
    "    ax.set_title(roi_name)\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Amplitude difference (¬µV)')\n",
    "axes[0].legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "roi_png = figdir / 'group_category_roi_diff.png'\n",
    "plt.savefig(roi_png, dpi=200, bbox_inches='tight')\n",
    "print(f\"üíæ Saved: {roi_png}\")\n",
    "plt.show()\n",
    "\n",
    "# Save ROI curves to CSV per ROI\n",
    "for roi_name, A_label, N_label in [('Frontal ROI','frontal_roi','frontal_roi'), ('Parieto-occipital ROI','parieto-occipital_roi','parieto-occipital_roi')]:\n",
    "    # reuse A,N from loop by re-computing quickly\n",
    "    # collect again to ensure variables exist here\n",
    "    subj_series_an = []\n",
    "    subj_series_non = []\n",
    "    for subj in sorted(set(roi_series[roi_name]['animal'].keys()) | set(roi_series[roi_name]['nonanimal'].keys())):\n",
    "        s_an = roi_series[roi_name]['animal'].get(subj)\n",
    "        s_non = roi_series[roi_name]['nonanimal'].get(subj)\n",
    "        if not s_an or not s_non:\n",
    "            continue\n",
    "        idx_union = sorted(set().union(*[s.index for s in (s_an+s_non)]))\n",
    "        an = np.nanmean(np.vstack([s.reindex(idx_union).values for s in s_an]), axis=0)\n",
    "        nn = np.nanmean(np.vstack([s.reindex(idx_union).values for s in s_non]), axis=0)\n",
    "        subj_series_an.append(pd.Series(an, index=idx_union))\n",
    "        subj_series_non.append(pd.Series(nn, index=idx_union))\n",
    "    if not subj_series_an or not subj_series_non:\n",
    "        continue\n",
    "    common_index = sorted(set().union(*[s.index for s in (subj_series_an+subj_series_non)]))\n",
    "    A = np.vstack([s.reindex(common_index).values for s in subj_series_an])\n",
    "    N = np.vstack([s.reindex(common_index).values for s in subj_series_non])\n",
    "    meanA = np.nanmean(A, axis=0); semA = np.nanstd(A, axis=0, ddof=1)/np.sqrt(np.sum(np.isfinite(A), axis=0).clip(min=1))\n",
    "    meanN = np.nanmean(N, axis=0); semN = np.nanstd(N, axis=0, ddof=1)/np.sqrt(np.sum(np.isfinite(N), axis=0).clip(min=1))\n",
    "    out_rows = []\n",
    "    for t, mA, sA, mN, sN in zip(common_index, meanA, semA, meanN, semN):\n",
    "        out_rows.append({'roi': roi_name, 'time_ms': float(t), 'category': 'animal', 'mean_uV': float(mA), 'sem_uV': float(sA)})\n",
    "        out_rows.append({'roi': roi_name, 'time_ms': float(t), 'category': 'nonanimal', 'mean_uV': float(mN), 'sem_uV': float(sN)})\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    out_csv = statdir / f'erp_category_roi_diff_{roi_name.replace(\" \", \"_\").lower()}.csv'\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    print(f\"üíæ Saved ROI curves ‚Üí {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69814f1e",
   "metadata": {},
   "source": [
    "## 9. Subject diagnostics ‚Äì sub-003\n",
    "\n",
    "Computes diagnostics to explain reduced SME/Cohen‚Äôs d:\n",
    "- Trial counts per condition and repetition\n",
    "- ROI channel availability and fallbacks\n",
    "- SME and Cohen‚Äôs d on trial-level ROI amplitudes (config post_window) before vs after cleaning\n",
    "- Approximate latency jitter (SD of single-trial peak latency in window)\n",
    "\n",
    "Saves:\n",
    "- `results/diagnostics/sub-003_diagnostics_summary.csv`\n",
    "- `results/diagnostics/sub-003_trial_metrics_<stage>_<roi>.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
